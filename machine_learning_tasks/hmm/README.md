# 隐马尔科夫模型  HMM 

HMM 是一种最简单的动态贝叶斯网，主要用于时序数据建模，在语音识别、自然语言处理等领域有广泛的应用
既是隐形，说明这些状态是观测不到的，相应的，我们可以通过其他方式来『猜测』或是『推断』这些状态，这也是 HMM 需要解决的问题之一。

## 五个基本要素

λ = (S, O, Pi, A, B)

S：状态集合
O：观察集合
Pi：初始状态概率
A：状态转移概率矩阵
B：给定状态下，观察值概率矩阵

## 三个假设

  1、有限历史性假设，p(si|si-1,si-2,...,s1) = p(si|si-1)
  2、齐次性假设，（状态与具体时间无关），P(si+1|si)=p(sj+1,sj)
  3、输出独立性假设，输出仅与当前状态有关，P(o1,...ot|s1,...st) = P(ot|st)

## HMM 解决的三个问题


### 评估问题

已知模型参数 λ = (A, B, π),计算某个观测序列发生的概率，即求P(O|λ) 

解决算法：
    1. 遍历所有可能的状态序列，计算每种情况的概率和就是所要求的概率
    2. 前向算法/后向算法

>> 参考资料： https://www.zhihu.com/question/20962240

### 解码问题

给出观测序列 O 和模型 λ，怎样选择一个状态序列S(s1,s2,...st+1),能最好的解释观测序列 O.

解决算法：

维比特算法。维特比算法致力于寻找一条最佳路径，以便能最好地解释观测到的序列。

我们假设第i个分类标记只依赖于第i-1个标记，那么我们可以得到分类标记的马尔科夫链了：

  P(C1，C2，C3….Ci) =P(C1)P(C2|C1)P(C3|C2)…P(Ci|Ci-1)

我们要找的是：
  argmax S  P(s1，s2，s3….si | O1, O2, O3……Oi) = P(S|O)....（1）

给定了O的情况下，最有可能出现的S的序列，也是HMM的“解码问题”，根据贝叶斯公式有：
  P(S|O) = P(O|S)P(S) / P(O)
由于P(O)对于每一种序列来说都是一样的，因此可以去掉，因此，（1）等价于 ：

  argmax S P(O|S)P(S)...............................（2）

P(S)的求法上面已经讲过了，对于P(O|S)，HMM有两个假设：
  （a）已知类的序列，观察值在统计上是独立的； 
  （b）某类的概率密度函数不依赖其他类。也就是说依赖性仅仅存在于产生类的序列，而在类内，观察值服从类自己的规则。因此对于：

    P(O|S) = P(O1, O2, O3……Oi | S1，S2，S3….Si )

根据假设a，观察序列具有统计意义上的独立性，因此：

  P(O1, O2, O3……Oi | S1，S2，S3….Si )=P(O1|S) * P(O2|S)……P(Oi|S)

根据假设b，对于Oi的分类Si与Sj没有依赖关系，所以有：

  P(O1|S) * P(O2|S)……P(Oi|S) = P(O1|S1) * P(O2|S2)……P(Oi|Si)

同样的P(Oi|Si) 可以通过在大量语料中使用统计的方法求得。至此我们在一个马尔科夫链上加入了观察值数据，构造了一个HMM，这个模型已经可以具备一定的分词能力啦！

尽管没有用到，顺便回顾一下HMM要解决的第一个问题“评估”，也即求一个给定观察序列的概率，即：P(O)，用全概率公式展开有：
  P(O) = sigmaO P(O|S)P(S)（sigmaO代表对O的集合求和），
同样需要计算P(O|S)，因此计算上和“解码”有相似之处。

>> 参考：http://www.52nlp.cn/itenyh%e7%89%88-%e7%94%a8hmm%e5%81%9a%e4%b8%ad%e6%96%87%e5%88%86%e8%af%8d%e4%b8%80%ef%bc%9a%e6%a8%a1%e5%9e%8b%e5%87%86%e5%a4%87

http://www.52nlp.cn/itenyh%e7%89%88-%e7%94%a8hmm%e5%81%9a%e4%b8%ad%e6%96%87%e5%88%86%e8%af%8d%e4%ba%94%ef%bc%9a%e4%b8%80%e4%b8%aa%e6%b7%b7%e5%90%88%e7%9a%84%e5%88%86%e8%af%8d%e5%99%a8

### 学习问题

如何调整模型参数 λ=(π, A, B), 使得P(O|λ)最大？  




其他：

机器学习笔记
  http://blog.csdn.net/ppn029012

  EM 算法： http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html

